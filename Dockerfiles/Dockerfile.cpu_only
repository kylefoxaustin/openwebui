# Build Ollama from source (CPU-only)
FROM golang:1.24-bullseye as ollama-builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Clone Ollama repository
WORKDIR /go/src
RUN git clone https://github.com/ollama/ollama.git

# Build Ollama with CPU-only flags
WORKDIR /go/src/ollama
RUN go mod download && \
    TARGETARCH=$(uname -m | sed -e 's/x86_64/amd64/' -e 's/aarch64/arm64/') && \
    echo "Building for architecture: $TARGETARCH" && \
    CGO_ENABLED=1 GOARCH=$TARGETARCH go build -o ollama .

# Build OpenWebUI frontend
FROM node:20-slim as frontend-builder
WORKDIR /app

# Install git
RUN apt-get update && apt-get install -y git && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Clone OpenWebUI repository
RUN git clone --depth 1 https://github.com/open-webui/open-webui.git .

# Build frontend
RUN npm ci && npm run build && npm cache clean --force

# Final stage
FROM python:3.11-slim

WORKDIR /app

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    curl \
    libmagic1 \
    poppler-utils \
    ffmpeg libsm6 libxext6 \
    netcat-openbsd \
    supervisor \
    bash \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Copy Ollama from builder
COPY --from=ollama-builder /go/src/ollama/ollama /usr/local/bin/ollama
RUN chmod +x /usr/local/bin/ollama

# Copy OpenWebUI files from frontend-builder
COPY --from=frontend-builder /app/build /app/build
COPY --from=frontend-builder /app/backend /app/backend
COPY --from=frontend-builder /app/CHANGELOG.md /app/
COPY --from=frontend-builder /app/package.json /app/

# Install Python dependencies
RUN pip install --no-cache-dir -r /app/backend/requirements.txt && \
    pip install --no-cache-dir uvicorn

# Create data directories
RUN mkdir -p /app/backend/data && chmod -R 777 /app/backend/data && \
    mkdir -p /root/.ollama && chmod -R 777 /root/.ollama

# Set environment variable to force CPU usage
ENV OLLAMA_HOST="0.0.0.0" \
    PORT=8080 \
    HOST=0.0.0.0 \
    OLLAMA_BASE_URL="http://localhost:11434"

# Set up supervisor configuration
RUN echo '[supervisord]\n\
nodaemon=true\n\
\n\
[program:ollama]\n\
command=/usr/local/bin/ollama serve\n\
environment=CUDA_VISIBLE_DEVICES="",OLLAMA_HOST="0.0.0.0"\n\
\n\
[program:openwebui]\n\
command=bash -c "cd /app/backend && python -m uvicorn open_webui.main:app --host 0.0.0.0 --port 8080"\n\
' > /etc/supervisor/conf.d/supervisord.conf

# Expose ports
EXPOSE 8080 11434

# Create volumes
VOLUME /root/.ollama
VOLUME /app/backend/data

CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/conf.d/supervisord.conf"]
